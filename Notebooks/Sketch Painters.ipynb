{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Sketch Painters.ipynb","provenance":[{"file_id":"1EezauMN_q0ssm8JOWn9SFhgsRsTX-k9Q","timestamp":1617102949391},{"file_id":"1PLYXnwHDtXAvFP6Bi1ZjYCkmcPQK4HpH","timestamp":1617100005977},{"file_id":"1ehtbqsxPXZ2Nd_iUca2Glq3JBinEL0iv","timestamp":1616063884232}],"collapsed_sections":[],"authorship_tag":"ABX9TyPYe9uWJWqwtt2BAFY9u8GX"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"id":"jwbrepXlcH61","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618231078555,"user_tz":-120,"elapsed":49087,"user":{"displayName":"redemptor jr laceda taloma","photoUrl":"","userId":"15190277289652303639"}},"outputId":"04d6c321-ae26-4372-97ed-0f980894a2be"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4TxnN-NuafdY"},"source":["import os\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchvision import transforms\n","\n","import cv2\n","from google.colab.patches import cv2_imshow\n","\n","from PIL import Image"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LNj5EdtfcXg0","executionInfo":{"status":"ok","timestamp":1618231110781,"user_tz":-120,"elapsed":928,"user":{"displayName":"redemptor jr laceda taloma","photoUrl":"","userId":"15190277289652303639"}},"outputId":"f5843d17-5746-4186-9dc8-d61cdd55437a"},"source":["# Device configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["cpu\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"z7Cc1AGPvUY6"},"source":["### Scaling function\n","Function for rescaling to range [0, 1]. This is needed for displaying images."]},{"cell_type":"code","metadata":{"id":"dacUP2_qxQ_s"},"source":["def scale_0_1(x):\n","  return (x+1)/2"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t3lJlaJeQu6D"},"source":["# **Generators**"]},{"cell_type":"markdown","metadata":{"id":"tzRMZ_mw4xRT"},"source":["## Pix2Pix\n"," [Image-to-Image Translation with Conditional Adversarial Networks](https://arxiv.org/abs/1611.07004)\n","\n","Check also [SketchyGAN: Towards Diverse and Realistic Sketch to Image Synthesis](https://arxiv.org/abs/1611.07004)."]},{"cell_type":"code","metadata":{"id":"GGoXBI7D-1u9"},"source":["class Pix2Pix(nn.Module):\n","  def __init__(self, d = 64):     #(N,1,256,256)\n","    super(Pix2Pix, self).__init__()\n","    self.d = d\n","\n","    #Unet encoder\n","    self.conv1 = nn.Conv2d(1, d, 4, 2, 1)\n","    self.conv2 = nn.Conv2d(d, d*2, 4, 2, 1)\n","    self.conv2_bn = nn.BatchNorm2d(d*2)\n","    self.conv3 = nn.Conv2d(d*2, d*4, 4, 2, 1)\n","    self.conv3_bn = nn.BatchNorm2d(d*4)\n","    self.conv4 = nn.Conv2d(d*4, d*8, 4, 2, 1)\n","    self.conv4_bn = nn.BatchNorm2d(d*8)\n","    self.conv5 = nn.Conv2d(d*8, d*8, 4, 2, 1)\n","    self.conv5_bn = nn.BatchNorm2d(d*8)\n","    self.conv6 = nn.Conv2d(d*8, d*8, 4, 2, 1)\n","    self.conv6_bn = nn.BatchNorm2d(d*8)\n","    self.conv7 = nn.Conv2d(d*8, d*8, 4, 2, 1)\n","    self.conv7_bn = nn.BatchNorm2d(d*8)\n","    self.conv8 = nn.Conv2d(d*8, d*8, 4, 2, 1)\n","\n","\n","    #Unet decoder\n","    self.deconv1 = nn.ConvTranspose2d(d*8, d*8, 4, 2, 1)\n","    self.deconv1_bn = nn.BatchNorm2d(d*8)\n","    self.deconv2 = nn.ConvTranspose2d(d*8*2, d*8, 4, 2, 1)\n","    self.deconv2_bn = nn.BatchNorm2d(d*8)\n","    self.deconv3 = nn.ConvTranspose2d(d*8*2, d*8 , 4, 2, 1)\n","    self.deconv3_bn = nn.BatchNorm2d(d*8)\n","    self.deconv4 = nn.ConvTranspose2d(d*8*2, d*8, 4, 2, 1)\n","    self.deconv4_bn = nn.BatchNorm2d(d*8)\n","    self.deconv5 = nn.ConvTranspose2d(d*8*2, d*4, 4, 2, 1)\n","    self.deconv5_bn = nn.BatchNorm2d(d*4)\n","    self.deconv6 = nn.ConvTranspose2d(d*4*2 , d*2, 4, 2, 1)\n","    self.deconv6_bn = nn.BatchNorm2d(d*2)\n","    self.deconv7 = nn.ConvTranspose2d(d*2*2, d, 4, 2, 1)\n","    self.deconv7_bn = nn.BatchNorm2d(d)\n","    self.deconv8 = nn.ConvTranspose2d(d*2, 3, 4, 2, 1)\n","\n","    #Weight initialization\n","    self.weight_init()\n","\n","\n","\n","  def forward(self, input):\n","    e1 = F.leaky_relu(self.conv1(input), 0.2)\n","    e2 = F.leaky_relu(self.conv2_bn(self.conv2(e1)), 0.2)\n","    e3 = F.leaky_relu(self.conv3_bn(self.conv3(e2)), 0.2)\n","    e4 = F.leaky_relu(self.conv4_bn(self.conv4(e3)), 0.2)\n","    e5 = F.leaky_relu(self.conv5_bn(self.conv5(e4)), 0.2)\n","    e6 = F.leaky_relu(self.conv6_bn(self.conv6(e5)), 0.2)\n","    e7 = F.leaky_relu(self.conv7_bn(self.conv7(e6)), 0.2)\n","    e8 = F.relu(self.conv8(e7))\n","    d1 = F.dropout(self.deconv1_bn(self.deconv1(e8)), 0.5 , training=True)\n","    d1 = F.relu(torch.cat([d1, e7], 1))\n","    d2 = F.dropout(self.deconv2_bn(self.deconv2(d1)), 0.5, training=True)\n","    d2 = F.relu(torch.cat([d2, e6], 1))\n","    d3 = F.dropout(self.deconv3_bn(self.deconv3(d2)), 0.5 , training=True)\n","    d3 = F.relu(torch.cat([d3, e5], 1))\n","    d4 = self.deconv4_bn(self.deconv4(d3))\n","    d4 = F.relu(torch.cat([d4, e4], 1))\n","    d5 = self.deconv5_bn(self.deconv5(d4))\n","    d5 = F.relu(torch.cat([d5, e3], 1))\n","    d6 = self.deconv6_bn(self.deconv6(d5))\n","    d6 = F.relu(torch.cat([d6, e2], 1))\n","    d7 = self.deconv7_bn(self.deconv7(d6))\n","    d7 = F.relu(torch.cat([d7, e1], 1))\n","    d8 = self.deconv8(d7)\n","    output = torch.tanh(d8)\n","\n","    return output\n","\n","\n","\n","  def weight_init(self):\n","    for m in self.modules():\n","      if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n","        m.weight.data.normal_(0.0, 0.02)\n","        if m.bias.data is not None:\n","          m.bias.data.zero_()\n","        print(m, ': Weights initialized')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RhAR63mztIT-"},"source":["## StylePix2Pix\n","Combination of [Image-to-Image Translation with Conditional Adversarial Networks](https://arxiv.org/abs/1611.07004) (i.e. Pix2Pix) and [Conditional Image Synthesis With Auxiliary Classifier GANs](https://arxiv.org/abs/1610.09585) (i.e. ACGAN).\n","\n","Check also [SketchyGAN: Towards Diverse and Realistic Sketch to Image Synthesis](https://arxiv.org/abs/1611.07004)."]},{"cell_type":"code","metadata":{"id":"Im4fXze-uN2q"},"source":["class StylePix2Pix(nn.Module):\n","  def __init__(self, d = 64, n = 3):     #(N,1,256,256)\n","    super(StylePix2Pix, self).__init__()\n","    self.d = d\n","\n","    #Concatenate Label\n","    self.embedding = nn.Embedding(n, 100)\n","    self.linear = nn.Linear(100, d*4*d*4)\n","\n","    #Unet encoder\n","    self.conv1 = nn.Conv2d(2, d, 4, 2, 1)\n","    self.conv2 = nn.Conv2d(d, d*2, 4, 2, 1)\n","    self.conv2_bn = nn.BatchNorm2d(d*2)\n","    self.conv3 = nn.Conv2d(d*2, d*4, 4, 2, 1)\n","    self.conv3_bn = nn.BatchNorm2d(d*4)\n","    self.conv4 = nn.Conv2d(d*4, d*8, 4, 2, 1)\n","    self.conv4_bn = nn.BatchNorm2d(d*8)\n","    self.conv5 = nn.Conv2d(d*8, d*8, 4, 2, 1)\n","    self.conv5_bn = nn.BatchNorm2d(d*8)\n","    self.conv6 = nn.Conv2d(d*8, d*8, 4, 2, 1)\n","    self.conv6_bn = nn.BatchNorm2d(d*8)\n","    self.conv7 = nn.Conv2d(d*8, d*8, 4, 2, 1)\n","    self.conv7_bn = nn.BatchNorm2d(d*8)\n","    self.conv8 = nn.Conv2d(d*8, d*8, 4, 2, 1)\n","\n","\n","    #Unet decoder\n","    self.deconv1 = nn.ConvTranspose2d(d*8, d*8, 4, 2, 1)\n","    self.deconv1_bn = nn.BatchNorm2d(d*8)\n","    self.deconv2 = nn.ConvTranspose2d(d*8*2, d*8, 4, 2, 1)\n","    self.deconv2_bn = nn.BatchNorm2d(d*8)\n","    self.deconv3 = nn.ConvTranspose2d(d*8*2, d*8 , 4, 2, 1)\n","    self.deconv3_bn = nn.BatchNorm2d(d*8)\n","    self.deconv4 = nn.ConvTranspose2d(d*8*2, d*8, 4, 2, 1)\n","    self.deconv4_bn = nn.BatchNorm2d(d*8)\n","    self.deconv5 = nn.ConvTranspose2d(d*8*2, d*4, 4, 2, 1)\n","    self.deconv5_bn = nn.BatchNorm2d(d*4)\n","    self.deconv6 = nn.ConvTranspose2d(d*4*2 , d*2, 4, 2, 1)\n","    self.deconv6_bn = nn.BatchNorm2d(d*2)\n","    self.deconv7 = nn.ConvTranspose2d(d*2*2, d, 4, 2, 1)\n","    self.deconv7_bn = nn.BatchNorm2d(d)\n","    self.deconv8 = nn.ConvTranspose2d(d*2, 3, 4, 2, 1)\n","\n","    #Weight initialization\n","    self.weight_init()\n","\n","\n","\n","  def forward(self, input, label):\n","    \n","    embedding = self.embedding(label)\n","    linear = self.linear(embedding)\n","\n","    merged =  torch.cat((input, torch.reshape(linear, (-1, 1, self.d*4, self.d*4))), 1)\n","\n","    e1 = F.leaky_relu(self.conv1(merged), 0.2)\n","    e2 = F.leaky_relu(self.conv2_bn(self.conv2(e1)), 0.2)\n","    e3 = F.leaky_relu(self.conv3_bn(self.conv3(e2)), 0.2)\n","    e4 = F.leaky_relu(self.conv4_bn(self.conv4(e3)), 0.2)\n","    e5 = F.leaky_relu(self.conv5_bn(self.conv5(e4)), 0.2)\n","    e6 = F.leaky_relu(self.conv6_bn(self.conv6(e5)), 0.2)\n","    e7 = F.leaky_relu(self.conv7_bn(self.conv7(e6)), 0.2)\n","    e8 = F.relu(self.conv8(e7))\n","    d1 = F.dropout(self.deconv1_bn(self.deconv1(e8)), 0.5 , training=True)\n","    d1 = F.relu(torch.cat([d1, e7], 1))\n","    d2 = F.dropout(self.deconv2_bn(self.deconv2(d1)), 0.5, training=True)\n","    d2 = F.relu(torch.cat([d2, e6], 1))\n","    d3 = F.dropout(self.deconv3_bn(self.deconv3(d2)), 0.5 , training=True)\n","    d3 = F.relu(torch.cat([d3, e5], 1))\n","    d4 = self.deconv4_bn(self.deconv4(d3))\n","    d4 = F.relu(torch.cat([d4, e4], 1))\n","    d5 = self.deconv5_bn(self.deconv5(d4))\n","    d5 = F.relu(torch.cat([d5, e3], 1))\n","    d6 = self.deconv6_bn(self.deconv6(d5))\n","    d6 = F.relu(torch.cat([d6, e2], 1))\n","    d7 = self.deconv7_bn(self.deconv7(d6))\n","    d7 = F.relu(torch.cat([d7, e1], 1))\n","    d8 = self.deconv8(d7)\n","    output = torch.tanh(d8)\n","\n","    return output\n","\n","\n","\n","  def weight_init(self):\n","    for m in self.modules():\n","      if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n","        m.weight.data.normal_(0.0, 0.02)\n","        if m.bias.data is not None:\n","          m.bias.data.zero_()\n","        print(m, ': Weights initialized')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XPfO3n3rxJqT"},"source":["## CycleGAN\n","[Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks](https://arxiv.org/abs/1703.10593)"]},{"cell_type":"code","metadata":{"id":"t01z1ISSxVcc"},"source":["class CycleGAN(nn.Module):\n","  def __init__(self):     #(N,3,256,256)\n","    super(CycleGAN, self).__init__()\n","\n","    self.c7s1_64 = nn.Sequential(\n","        nn.ReflectionPad2d(3),\n","        nn.Conv2d(3, 64, 7, 1, 0),\n","        nn.InstanceNorm2d(64),\n","        nn.ReLU(True)\n","    )\n","\n","    self.d128 = nn.Sequential(\n","        nn.Conv2d(64, 128, 3, 2, 1),\n","        nn.InstanceNorm2d(128),\n","        nn.ReLU(True)\n","    )\n","\n","    self.d256 = nn.Sequential(\n","        nn.Conv2d(128, 256, 3, 2, 1),\n","        nn.InstanceNorm2d(256),\n","        nn.ReLU(True)\n","    )\n","\n","    self.r256_1 = nn.Sequential(\n","        nn.ReflectionPad2d(1),\n","        nn.Conv2d(256, 256, 3, 1, 0),\n","        nn.InstanceNorm2d(256),\n","        nn.ReLU(True),\n","        nn.ReflectionPad2d(1),\n","        nn.Conv2d(256, 256, 3, 1, 0),\n","        nn.InstanceNorm2d(256)\n","    )\n","\n","    self.r256_2 = nn.Sequential(\n","        nn.ReflectionPad2d(1),\n","        nn.Conv2d(256, 256, 3, 1, 0),\n","        nn.InstanceNorm2d(256),\n","        nn.ReLU(True),\n","        nn.ReflectionPad2d(1),\n","        nn.Conv2d(256, 256, 3, 1, 0),\n","        nn.InstanceNorm2d(256)\n","    )\n","\n","    self.r256_3 = nn.Sequential(\n","        nn.ReflectionPad2d(1),\n","        nn.Conv2d(256, 256, 3, 1, 0),\n","        nn.InstanceNorm2d(256),\n","        nn.ReLU(True),\n","        nn.ReflectionPad2d(1),\n","        nn.Conv2d(256, 256, 3, 1, 0),\n","        nn.InstanceNorm2d(256)\n","    )\n","\n","    self.r256_4 = nn.Sequential(\n","        nn.ReflectionPad2d(1),\n","        nn.Conv2d(256, 256, 3, 1, 0),\n","        nn.InstanceNorm2d(256),\n","        nn.ReLU(True),\n","        nn.ReflectionPad2d(1),\n","        nn.Conv2d(256, 256, 3, 1, 0),\n","        nn.InstanceNorm2d(256)\n","    )\n","\n","    self.r256_5 = nn.Sequential(\n","        nn.ReflectionPad2d(1),\n","        nn.Conv2d(256, 256, 3, 1, 0),\n","        nn.InstanceNorm2d(256),\n","        nn.ReLU(True),\n","        nn.ReflectionPad2d(1),\n","        nn.Conv2d(256, 256, 3, 1, 0),\n","        nn.InstanceNorm2d(256)\n","    )\n","\n","    self.r256_6 = nn.Sequential(\n","        nn.ReflectionPad2d(1),\n","        nn.Conv2d(256, 256, 3, 1, 0),\n","        nn.InstanceNorm2d(256),\n","        nn.ReLU(True),\n","        nn.ReflectionPad2d(1),\n","        nn.Conv2d(256, 256, 3, 1, 0),\n","        nn.InstanceNorm2d(256)\n","    )\n","\n","    self.r256_7 = nn.Sequential(\n","        nn.ReflectionPad2d(1),\n","        nn.Conv2d(256, 256, 3, 1, 0),\n","        nn.InstanceNorm2d(256),\n","        nn.ReLU(True),\n","        nn.ReflectionPad2d(1),\n","        nn.Conv2d(256, 256, 3, 1, 0),\n","        nn.InstanceNorm2d(256)\n","    )\n","\n","    self.r256_8 = nn.Sequential(\n","        nn.ReflectionPad2d(1),\n","        nn.Conv2d(256, 256, 3, 1, 0),\n","        nn.InstanceNorm2d(256),\n","        nn.ReLU(True),\n","        nn.ReflectionPad2d(1),\n","        nn.Conv2d(256, 256, 3, 1, 0),\n","        nn.InstanceNorm2d(256)\n","    )\n","\n","    self.r256_9 = nn.Sequential(\n","        nn.ReflectionPad2d(1),\n","        nn.Conv2d(256, 256, 3, 1, 0),\n","        nn.InstanceNorm2d(256),\n","        nn.ReLU(True),\n","        nn.ReflectionPad2d(1),\n","        nn.Conv2d(256, 256, 3, 1, 0),\n","        nn.InstanceNorm2d(256)\n","    )\n","\n","    self.u128 = nn.Sequential(\n","        nn.ConvTranspose2d(256, 128, 3, 2, 1, output_padding=1),\n","        nn.InstanceNorm2d(128),\n","        nn.ReLU(True)\n","    )\n","\n","    self.u64 = nn.Sequential(\n","        nn.ConvTranspose2d(128, 64, 3, 2, 1, output_padding=1),\n","        nn.InstanceNorm2d(64),\n","        nn.ReLU(True)\n","    )\n","\n","    self.c7s1_3 = nn.Sequential(\n","        nn.ReflectionPad2d(3),\n","        nn.Conv2d(64, 3, 7, 1, 0),\n","        nn.Tanh()\n","    )\n","\n","    self.weight_init()\n","\n","\n","\n","  def forward(self, input):\n","    x = self.c7s1_64(input)\n","\n","    x = self.d128(x)\n","    x = self.d256(x)\n","\n","    x = self.r256_1(x) + x\n","    x = self.r256_2(x) + x\n","    x = self.r256_3(x) + x\n","    x = self.r256_4(x) + x\n","    x = self.r256_5(x) + x\n","    x = self.r256_6(x) + x\n","    x = self.r256_7(x) + x\n","    x = self.r256_8(x) + x\n","    x = self.r256_9(x) + x\n","\n","    x = self.u128(x)\n","    x = self.u64(x)\n","\n","    out = self.c7s1_3(x)\n","\n","    return out\n","\n","\n","\n","  def weight_init(self):\n","\n","    def normal_init(m):\n","      # Conv2d, ConvTranspose2d\n","      if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n","        m.weight.data.normal_(0.0, 0.02)\n","        if m.bias.data is not None:\n","          m.bias.data.zero_()\n","        print(m, ': Weights initialized')\n","\n","    for block in self._modules:\n","      try:\n","        for m in self._modules[block]:\n","          normal_init(m)\n","      except:\n","        pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_i7X2DqMy_m4"},"source":["# **Test**"]},{"cell_type":"markdown","metadata":{"id":"-yLXXNff_pTX"},"source":["## On pictures in folders"]},{"cell_type":"code","metadata":{"id":"z-5OJjojuJPM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618231485644,"user_tz":-120,"elapsed":414,"user":{"displayName":"redemptor jr laceda taloma","photoUrl":"","userId":"15190277289652303639"}},"outputId":"7a7d55fb-efe7-4f86-fe56-ee3d46681d18"},"source":["PATH = '/content/gdrive/MyDrive/VanGoghGAN_Landscapes/Test'\n","#folders = sorted(os.listdir(PATH))\n","folders = ['Constable', 'Drawings', 'Lucien Pissarro', 'Matisse', 'Photos', 'Rousseau', 'Sargent', 'Seurat', 'Van Gogh Drawings']\n","print(folders)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['Constable', 'Drawings', 'Lucien Pissarro', 'Matisse', 'Photos', 'Rousseau', 'Sargent', 'Seurat', 'Van Gogh Drawings']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"r9Kjchsa0bp0"},"source":["#### Pix2Pix"]},{"cell_type":"code","metadata":{"id":"anYvr9Lc0eQ1"},"source":["# to hide output of this cell\n","%%capture\n","\n","CHECKPOINTS_FOLDER = '/content/gdrive/MyDrive/VanGoghGAN_Landscapes/Pix2Pix_Checkpoints'\n","\n","pix2pix = Pix2Pix()\n","pix2pix.to(device)\n","\n","epoch = 20\n","pix2pix.load_state_dict(torch.load(CHECKPOINTS_FOLDER+'/generator_{}.ckpt'.format(epoch), map_location=torch.device(device)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G_mnAoOE0hzb"},"source":["for folder in folders:\n","  print(folder+'\\n')\n","  sketch_dataset = torch.load(PATH+'/'+folder+'/'+folder+' Sketches.pt')\n","  for (i,filename) in enumerate(os.listdir(PATH+'/'+folder+'/Originals')):\n","    pred = scale_0_1(pix2pix(sketch_dataset[i:i+1].to(device)).squeeze())\n","    image = transforms.ToPILImage()(pred).convert('RGB')\n","    display(image)\n","    print(filename+'\\n')\n","    image.save(PATH + '/' + folder + '/Images_Pix2Pix/' + filename)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NZeK6NLhF3rU"},"source":["#### StylePix2Pix"]},{"cell_type":"code","metadata":{"id":"HV67D36qF7Qz"},"source":["# to hide output of this cell\n","%%capture\n","\n","CHECKPOINTS_FOLDER = '/content/gdrive/MyDrive/VanGoghGAN_Landscapes/StylePix2Pix_Checkpoints'\n","\n","stylepix2pix = StylePix2Pix()\n","stylepix2pix.to(device)\n","\n","epoch = 50\n","stylepix2pix.load_state_dict(torch.load(CHECKPOINTS_FOLDER+'/generator_{}.ckpt'.format(epoch), map_location=torch.device(device)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1dMmQpnQSeCGG0rTF2AwmQ_oW87R9DM7K"},"id":"GZf1Pv6ZF-G7","executionInfo":{"status":"ok","timestamp":1618231472443,"user_tz":-120,"elapsed":103092,"user":{"displayName":"redemptor jr laceda taloma","photoUrl":"","userId":"15190277289652303639"}},"outputId":"a46208a1-2f7c-4d0e-9098-4ed384579a67"},"source":["for folder in folders:\n","  print(folder+'\\n')\n","  sketch_dataset = torch.load(PATH+'/'+folder+'/'+folder+' Sketches.pt')\n","  for (i,filename) in enumerate(os.listdir(PATH+'/'+folder+'/Originals')):\n","    # 0: Monet\n","    pred = scale_0_1(stylepix2pix(sketch_dataset[i:i+1].to(device), torch.LongTensor([0]).to(device)).squeeze())\n","    image = transforms.ToPILImage()(pred).convert('RGB')\n","    display(image)\n","    print(filename+'\\n')\n","    image.save(PATH + '/' + folder + '/Images_StylePix2Pix/' + filename[:-4] + ' 0_Monet.jpg')\n","\n","    # 1: Van Gogh\n","    pred = scale_0_1(stylepix2pix(sketch_dataset[i:i+1].to(device), torch.LongTensor([1]).to(device)).squeeze())\n","    image = transforms.ToPILImage()(pred).convert('RGB')\n","    display(image)\n","    print(filename+'\\n')\n","    image.save(PATH + '/' + folder + '/Images_StylePix2Pix/' + filename[:-4] + ' 1_Van_Gogh.jpg')\n","\n","    # 2: Corot and Shishkin\n","    pred = scale_0_1(stylepix2pix(sketch_dataset[i:i+1].to(device), torch.LongTensor([2]).to(device)).squeeze())\n","    image = transforms.ToPILImage()(pred).convert('RGB')\n","    display(image)\n","    print(filename+'\\n')\n","    image.save(PATH + '/' + folder + '/Images_StylePix2Pix/' + filename[:-4] + ' 2_Corot_Shishkin.jpg')"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"fmFE1hreMgRc"},"source":["#### CycleGAN"]},{"cell_type":"code","metadata":{"id":"5eHn85b1zcQF"},"source":["# to hide output of this cell\n","%%capture\n","\n","CHECKPOINTS_FOLDER = '/content/gdrive/MyDrive/VanGoghGAN_Landscapes/CycleGAN_Checkpoints'\n","\n","cyclegan = CycleGAN()\n","cyclegan.to(device)\n","\n","epoch = 102\n","cyclegan.load_state_dict(torch.load(CHECKPOINTS_FOLDER+'/generatorS2P_{}.ckpt'.format(epoch), map_location=torch.device(device)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r0wzok9pQe3y"},"source":["for folder in folders:\n","  print(folder+'\\n')\n","  sketch_dataset = torch.load(PATH+'/'+folder+'/'+folder+' Sketches 3D.pt')\n","  for (i,filename) in enumerate(os.listdir(PATH+'/'+folder+'/Originals')):\n","    pred = scale_0_1(cyclegan(sketch_dataset[i:i+1].to(device)).squeeze())\n","    image = transforms.ToPILImage()(pred).convert('RGB')\n","    display(image)\n","    print(filename+'\\n')\n","    image.save(PATH + '/' + folder + '/Images_CycleGAN/' + filename)"],"execution_count":null,"outputs":[]}]}